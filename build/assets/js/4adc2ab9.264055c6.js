"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[601],{5744(r,e,n){n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"index","title":"Introduction","description":"What is Fraud Detection?","source":"@site/My-docs/index.md","sourceDirName":".","slug":"/","permalink":"/fraud-detector/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Introduction","sidebar_label":"Introduction"},"sidebar":"tutorialSidebar","next":{"title":"Introduction","permalink":"/fraud-detector/"}}');var t=n(4848),s=n(8453);const i={title:"Introduction",sidebar_label:"Introduction"},o="Introduction to Fraud Detection",l={},c=[{value:"Understanding the problem",id:"understanding-the-problem",level:4},{value:"Complete walkthrough",id:"complete-walkthrough",level:4}];function d(r){const e={code:"code",h1:"h1",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...r.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"introduction-to-fraud-detection",children:"Introduction to Fraud Detection"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"What is Fraud Detection?"})}),"\n",(0,t.jsx)(e.p,{children:'Fraud detection is the process of identifying fraudulent transactions in real-time before they cause financial damage. Think of it as a security guard for money - every time someone tries to use a credit card, make a transfer, or complete a payment, our system needs to decide in milliseconds: "Is this legitimate or fraudulent?"'}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Why Machine Learning?"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'IF transaction_amount > $10,000 \r\nAND country = "Nigeria" \r\nTHEN block\n'})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Problems with rule-based:"})}),"\n",(0,t.jsx)(e.p,{children:"Fraudsters learn the rules and bypass them"}),"\n",(0,t.jsx)(e.p,{children:"Thousands of manual rules to maintain"}),"\n",(0,t.jsx)(e.p,{children:"Can't adapt to new fraud patterns quickly"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"ML-based systems learn patterns automatically:"})}),"\n",(0,t.jsx)(e.p,{children:"Detect complex relationships humans miss"}),"\n",(0,t.jsx)(e.p,{children:"Adapt to new fraud patterns"}),"\n",(0,t.jsx)(e.p,{children:"Reduce false positives (legitimate transactions wrongly blocked)"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Real-World Impact"})}),"\n",(0,t.jsx)(e.p,{children:"In 2023, payment fraud losses reached $38 billion globally. A good ML system can:"}),"\n",(0,t.jsx)(e.p,{children:"Block 80-90% of fraudulent transactions"}),"\n",(0,t.jsx)(e.p,{children:"Reduce false positives by 50-70%"}),"\n",(0,t.jsx)(e.p,{children:"Save millions in chargeback fees"}),"\n",(0,t.jsx)(e.p,{children:"Protect customer trust"}),"\n",(0,t.jsx)(e.p,{children:"This is exactly what we'll build \u2014 a full machine learning system that goes from raw transaction data \u2192 trained model \u2192 real-time fraud scoring.\r\nin the image, the top part shows the old rule-based way (lots of manual work, easy to fool). The bottom part is the modern ML way (automatically learns patterns and keeps improving)."}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{alt:"Alt text",src:n(8741).A+"",width:"1120",height:"684"})}),"\n",(0,t.jsx)(e.h4,{id:"understanding-the-problem",children:"Understanding the problem"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"The Dataset We'll Use"})}),"\n",(0,t.jsx)(e.p,{children:"Credit Card Fraud Detection Dataset (ULB, 2013)"}),"\n",(0,t.jsx)(e.p,{children:"284,807 transactions from European cardholders"}),"\n",(0,t.jsx)(e.p,{children:"492 frauds (only 0.172% of total)"}),"\n",(0,t.jsx)(e.p,{children:"31 columns: Time, V1-V28 (PCA transformed), Amount, Class"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"The Challenge: Extreme Class Imbalance"})}),"\n",(0,t.jsx)(e.p,{children:"Genuine transactions: 284,315 (99.828%)\r\nFraudulent transactions: 492 (0.172%)"}),"\n",(0,t.jsx)(e.p,{children:"This imbalance is the fundamental challenge of fraud detection. If you predict \"genuine\" for every transaction, you're 99.8% accurate but completely useless - you'd miss all fraud!"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Real Constraints"})}),"\n",(0,t.jsxs)(e.p,{children:["(1).",(0,t.jsx)(e.strong,{children:"Speed:"})," Must score in ",(0,t.jsx)(e.code,{children:"<100ms"})," (customer won't wait)"]}),"\n",(0,t.jsxs)(e.p,{children:["(2). ",(0,t.jsx)(e.strong,{children:"Interpretability:"})," Must explain WHY a transaction was flagged"]}),"\n",(0,t.jsxs)(e.p,{children:["(3). ",(0,t.jsx)(e.strong,{children:"Adaptability:"})," Fraud patterns change constantly"]}),"\n",(0,t.jsxs)(e.p,{children:["(4). ",(0,t.jsx)(e.strong,{children:"Regulatory:"})," Must comply with banking regulations"]}),"\n",(0,t.jsx)(e.h4,{id:"complete-walkthrough",children:"Complete walkthrough"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Environment setup"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Create isolated environment (prevents package conflicts)\r\nconda create -n fraud_detection python=3.11 -y\r\nconda activate fraud_detection\r\n\r\n# Install essential libraries\r\npip install pandas numpy scikit-learn matplotlib seaborn\r\npip install lightgbm xgboost imbalanced-learn\r\npip install shap jupyter fastapi uvicorn joblib\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"What each library does:"})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"pandas/numpy:"})," Data manipulation (like Excel on steroids)"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"scikit-learn:"})," Machine learning algorithms and tools"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"matplotlib:"})," Create visualizations (understand your data)"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"lightgbm"}),": State-of-the-art ML algorithms"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"imbalanced-learn:"})," Handle the fraud/genuine imbalance"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"shap:"})," Explain why model flagged transactions"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"uvicorn:"})," Turn model into web service"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"joblib:"})," load trained models"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Data loading and exploration"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\n# Load data\r\ndf = pd.read_csv('creditcard.csv')\r\n\r\n# 1. BASIC INFO\r\nprint(\"Dataset Shape:\", df.shape)  # (284807, 31)\r\nprint(\"\\nColumns:\", df.columns.tolist())\r\nprint(\"\\nData Types:\\n\", df.dtypes)\r\nprint(\"\\nMissing Values:\\n\", df.isnull().sum())\r\n\r\n# 2. TARGET DISTRIBUTION (The Imbalance)\r\nprint(\"\\nClass Distribution:\")\r\nprint(df['Class'].value_counts())\r\nprint(f\"Fraud Rate: {df['Class'].mean()*100:.4f}%\")\r\n\r\n# Visualize imbalance\r\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n\r\n# Bar plot\r\nsns.countplot(x='Class', data=df, ax=axes[0])\r\naxes[0].set_title('Transaction Distribution')\r\naxes[0].set_xticklabels(['Genuine', 'Fraud'])\r\n\r\n# Pie chart\r\ndf['Class'].value_counts().plot.pie(\r\n    autopct='%1.2f%%', \r\n    labels=['Genuine', 'Fraud'],\r\n    ax=axes[1]\r\n)\r\naxes[1].set_title('Class Distribution')\r\n\r\nplt.tight_layout()\r\nplt.show()\r\n\r\n# 3. STATISTICAL SUMMARY\r\nprint(\"\\nAmount Statistics by Class:\")\r\nprint(df.groupby('Class')['Amount'].describe())\r\n\r\n# 4. TIME ANALYSIS\r\ndf['Hour'] = (df['Time'] / 3600) % 24  # Convert seconds to hour of day\r\n\r\nplt.figure(figsize=(12, 4))\r\nplt.subplot(1, 2, 1)\r\nsns.histplot(data=df[df['Class']==0], x='Hour', bins=24, alpha=0.5, label='Genuine')\r\nsns.histplot(data=df[df['Class']==1], x='Hour', bins=24, alpha=0.5, label='Fraud')\r\nplt.legend()\r\nplt.title('Transaction Time Distribution')\r\n\r\nplt.subplot(1, 2, 2)\r\nsns.boxplot(x='Class', y='Amount', data=df[df['Amount'] < 1000])  # Cap at 1000 for visibility\r\nplt.xticks([0, 1], ['Genuine', 'Fraud'])\r\nplt.title('Transaction Amount Distribution')\r\n\r\nplt.tight_layout()\r\nplt.show()\r\n\r\n# 5. CORRELATION ANALYSIS\r\nplt.figure(figsize=(16, 12))\r\ncorrelation_matrix = df.corr()\r\nsns.heatmap(correlation_matrix, cmap='RdBu_r', center=0, \r\n            annot=False, fmt='.2f', square=True)\r\nplt.title('Feature Correlations')\r\nplt.show()\r\n\r\n# Features most correlated with fraud\r\nfraud_correlations = correlation_matrix['Class'].sort_values(ascending=False)\r\nprint(\"\\nTop 10 Features Correlated with Fraud:\")\r\nprint(fraud_correlations[1:11])  # Skip Class itself\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"What we can learn from this:"})}),"\n",(0,t.jsx)(e.p,{children:"(a). Data is massively imbalanced (0.17% fraud)"}),"\n",(0,t.jsx)(e.p,{children:"(b). V1-V28 are anonymized (PCA transformed for privacy)"}),"\n",(0,t.jsx)(e.p,{children:"(c). Fraud transactions tend to be smaller amounts (hiding in plain sight)"}),"\n",(0,t.jsx)(e.p,{children:"(d). Fraud happens more at certain hours (when people sleep)"}),"\n",(0,t.jsx)(e.p,{children:"(e). Features have different scales (Amount vs PCA components)"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Data preprocessing"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from sklearn.preprocessing import StandardScaler\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n# Separate features and target\r\nX = df.drop(['Class', 'Hour'], axis=1)  # Drop target and derived features\r\ny = df['Class']\r\n\r\n# FEATURE ENGINEERING\r\n# 1. Scale Amount (important for some models)\r\nscaler = StandardScaler()\r\nX['Scaled_Amount'] = scaler.fit_transform(X[['Amount']])\r\nX = X.drop('Amount', axis=1)  # Replace original with scaled\r\n\r\n# 2. Create time-based features\r\nX['Hour_sin'] = np.sin(2 * np.pi * df['Hour']/24)\r\nX['Hour_cos'] = np.cos(2 * np.pi * df['Hour']/24)\r\n\r\n# 3. Add interaction features (domain knowledge)\r\nX['Amount_V1_interaction'] = X['Scaled_Amount'] * X['V1']\r\nX['Amount_V2_interaction'] = X['Scaled_Amount'] * X['V2']\r\n\r\n# 4. Create velocity features (simulated - in real data you'd use historical)\r\n# Here we simulate: transactions per hour for this card\r\nX['Transaction_velocity'] = df.groupby('Time')['Time'].transform('count')\r\n\r\n# TRAIN-TEST SPLIT (Critical: no data leakage!)\r\nX_train, X_test, y_train, y_test = train_test_split(\r\n    X, y, \r\n    test_size=0.2,           # 20% for testing\r\n    random_state=42,         # Reproducibility\r\n    stratify=y,              # Maintain fraud ratio\r\n    shuffle=True             # Randomize order\r\n)\r\n\r\nprint(f\"Train set: {X_train.shape}, Fraud rate: {y_train.mean():.4f}\")\r\nprint(f\"Test set: {X_test.shape}, Fraud rate: {y_test.mean():.4f}\")\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Why preprocessing matters:"})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Scaling:"})," Prevents features with large values from dominating"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Time encoding:"})," Captures cyclical nature of time (23:00 and 01:00 are close)"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Velocity:"})," Fraud often involves many transactions in short time"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"No leakage:"})," Test set must be completely unseen during training"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Handling Imbalanced Data"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from sklearn.linear_model import LogisticRegression\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nimport lightgbm as lgb\r\nfrom imblearn.over_sampling import SMOTE\r\nfrom imblearn.pipeline import Pipeline\r\nfrom sklearn.metrics import classification_report, confusion_matrix\r\n\r\n# Approach 1: Class Weights (Recommended for trees)\r\n# =================================================\r\n# Penalize misclassifying fraud more heavily\r\nmodel_rf_weighted = RandomForestClassifier(\r\n   n_estimators=200,\r\n   class_weight={0: 1, 1: 100},  # Fraud is 100x more important\r\n   max_depth=10,                   # Prevent overfitting\r\n   min_samples_split=5,            # Minimum samples to split node\r\n   min_samples_leaf=2,             # Minimum samples in leaf\r\n   random_state=42,\r\n   n_jobs=-1                       # Use all CPU cores\r\n)\r\n\r\n# Approach 2: SMOTE (Synthetic Minority Oversampling)\r\n# ====================================================\r\n# Create synthetic fraud examples\r\nsmote = SMOTE(random_state=42, sampling_strategy=0.1)  # Make fraud 10% of training\r\n\r\n# Create pipeline\r\npipeline_smote = Pipeline([\r\n   ('smote', SMOTE(random_state=42)),\r\n   ('classifier', RandomForestClassifier(\r\n       n_estimators=200,\r\n       max_depth=10,\r\n       random_state=42,\r\n       n_jobs=-1\r\n   ))\r\n])\r\n\r\n# Approach 3: Balanced Random Forest (Built-in sampling)\r\n# =======================================================\r\nmodel_rf_balanced = RandomForestClassifier(\r\n   n_estimators=200,\r\n   class_weight='balanced_subsample',  # Balance each bootstrap sample\r\n   max_depth=10,\r\n   random_state=42,\r\n   n_jobs=-1\r\n)\r\n\r\n# Approach 4: LightGBM with scale_pos_weight (Best in 2026)\r\n# ==========================================================\r\nscale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\r\n# scale_pos_weight \u2248 580 (genuine:fraqraud ratio)\r\n\r\nmodel_lgb = lgb.LGBMClassifier(\r\n   n_estimators=500,\r\n   learning_rate=0.05,\r\n   max_depth=8,\r\n   num_leaves=31,\r\n   scale_pos_weight=scale_pos_weight,  # Weight positive class\r\n   subsample=0.8,                       # Use 80% of data per tree\r\n   colsample_bytree=0.8,                 # Use 80% of features per tree\r\n   random_state=42,\r\n   n_jobs=-1,\r\n   verbose=-1\r\n)\r\n\r\n# Train all models to compare\r\nmodels = {\r\n   'Weighted RF': model_rf_weighted,\r\n   'SMOTE Pipeline': pipeline_smote,\r\n   'Balanced RF': model_rf_balanced,\r\n   'LightGBM': model_lgb\r\n}\r\n\r\nresults = {}\r\nfor name, model in models.items():\r\n   print(f\"\\nTraining {name}...\")\r\n   \r\n   if name != 'SMOTE Pipeline':  # Pipeline needs fit, not separate\r\n       model.fit(X_train, y_train)\r\n       pred = model.predict(X_test)\r\n   else:\r\n       model.fit(X_train, y_train)  # Pipeline handles SMOTE internally\r\n       pred = model.predict(X_test)\r\n   \r\n   results[name] = pred\r\n   \r\n   print(f\"Classification Report for {name}:\")\r\n   print(classification_report(y_test, pred, \r\n         target_names=['Genuine', 'Fraud']))\n"})}),"\n",(0,t.jsxs)(e.p,{children:["*",(0,t.jsx)(e.strong,{children:"Model evaluation and results"})]}),"\n",(0,t.jsx)(e.p,{children:"Now let's evaluate our models properly. For imbalanced fraud detection, accuracy is misleading - we need precision, recall, and F1-score."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from sklearn.metrics import (precision_recall_curve, average_precision_score, \r\n                            confusion_matrix, ConfusionMatrixDisplay, roc_auc_score)\r\n\r\ndef evaluate_models(models, X_test, y_test):\r\n    \"\"\"\r\n    Comprehensive model evaluation with business-focused metrics\r\n    \"\"\"\r\n    results = {}\r\n    \r\n    for name, model in models.items():\r\n        print(f\"\\n{'='*50}\")\r\n        print(f\"Evaluating {name}\")\r\n        print('='*50)\r\n        \r\n        # Get predictions\r\n        if name == 'SMOTE Pipeline':\r\n            y_pred = model.predict(X_test)\r\n            y_proba = model.predict_proba(X_test)[:, 1]\r\n        else:\r\n            y_pred = model.predict(X_test)\r\n            y_proba = model.predict_proba(X_test)[:, 1]\r\n        \r\n        # 1. Confusion Matrix\r\n        cm = confusion_matrix(y_test, y_pred)\r\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, \r\n                                     display_labels=['Genuine', 'Fraud'])\r\n        disp.plot()\r\n        plt.title(f'Confusion Matrix - {name}')\r\n        plt.show()\r\n        \r\n        # 2. Key Metrics\r\n        tn, fp, fn, tp = cm.ravel()\r\n        \r\n        metrics = {\r\n            'model': name,\r\n            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\r\n            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,\r\n            'f1_score': 2 * (tp / (tp + fp) * tp / (tp + fn)) / \r\n                       ((tp / (tp + fp)) + (tp / (tp + fn))) if (tp + fp) > 0 and (tp + fn) > 0 else 0,\r\n            'false_positive_rate': fp / (fp + tn),\r\n            'false_negative_rate': fn / (fn + tp),\r\n            'avg_precision': average_precision_score(y_test, y_proba),\r\n            'roc_auc': roc_auc_score(y_test, y_proba)\r\n        }\r\n        \r\n        # 3. Business Impact Calculations (assuming \u20ac100 avg fraud amount)\r\n        avg_fraud_amount = 100\r\n        total_frauds = len(y_test[y_test==1])\r\n        \r\n        metrics['business_impact'] = {\r\n            'frauds_caught': tp,\r\n            'frauds_missed': fn,\r\n            'money_saved': tp * avg_fraud_amount,\r\n            'money_lost': fn * avg_fraud_amount,\r\n            'false_alarms': fp,\r\n            'customers_inconvenienced': fp\r\n        }\r\n        \r\n        results[name] = metrics\r\n        \r\n        # Print results\r\n        print(f\"\\n\ud83d\udcca Performance Metrics for {name}:\")\r\n        print(f\"  Precision: {metrics['precision']:.4f} ({metrics['precision']*100:.1f}%)\")\r\n        print(f\"  Recall:    {metrics['recall']:.4f} ({metrics['recall']*100:.1f}%)\")\r\n        print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\r\n        print(f\"  Avg Precision (PR-AUC): {metrics['avg_precision']:.4f}\")\r\n        print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\r\n        \r\n        print(f\"\\n\ud83d\udcb0 Business Impact (test set, \u20ac100 avg fraud):\")\r\n        print(f\"  Frauds caught: {metrics['business_impact']['frauds_caught']} (Saved: \u20ac{metrics['business_impact']['money_saved']:,.0f})\")\r\n        print(f\"  Frauds missed: {metrics['business_impact']['frauds_missed']} (Lost: \u20ac{metrics['business_impact']['money_lost']:,.0f})\")\r\n        print(f\"  False alarms:  {metrics['business_impact']['false_alarms']} (Customers inconvenienced)\")\r\n    \r\n    return results\r\n\r\n# Run evaluation\r\nevaluation_results = evaluate_models(models, X_test, y_test)\r\n\r\n# Compare models visually\r\ndef plot_model_comparison(results):\r\n    \"\"\"Create comparison bar charts for all models\"\"\"\r\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\r\n    \r\n    models = list(results.keys())\r\n    \r\n    # Precision comparison\r\n    precisions = [results[m]['precision'] for m in models]\r\n    axes[0, 0].bar(models, precisions, color='skyblue')\r\n    axes[0, 0].set_title('Precision by Model', fontsize=14, fontweight='bold')\r\n    axes[0, 0].set_ylabel('Precision')\r\n    axes[0, 0].set_ylim([0, 1])\r\n    for i, v in enumerate(precisions):\r\n        axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center')\r\n    \r\n    # Recall comparison\r\n    recalls = [results[m]['recall'] for m in models]\r\n    axes[0, 1].bar(models, recalls, color='lightcoral')\r\n    axes[0, 1].set_title('Recall by Model', fontsize=14, fontweight='bold')\r\n    axes[0, 1].set_ylabel('Recall')\r\n    axes[0, 1].set_ylim([0, 1])\r\n    for i, v in enumerate(recalls):\r\n        axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center')\r\n    \r\n    # F1 comparison\r\n    f1_scores = [results[m]['f1_score'] for m in models]\r\n    axes[1, 0].bar(models, f1_scores, color='lightgreen')\r\n    axes[1, 0].set_title('F1-Score by Model', fontsize=14, fontweight='bold')\r\n    axes[1, 0].set_ylabel('F1-Score')\r\n    axes[1, 0].set_ylim([0, 1])\r\n    for i, v in enumerate(f1_scores):\r\n        axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center')\r\n    \r\n    # PR-AUC comparison (best metric for imbalanced data)\r\n    pr_aucs = [results[m]['avg_precision'] for m in models]\r\n    axes[1, 1].bar(models, pr_aucs, color='gold')\r\n    axes[1, 1].set_title('PR-AUC by Model', fontsize=14, fontweight='bold')\r\n    axes[1, 1].set_ylabel('PR-AUC')\r\n    axes[1, 1].set_ylim([0, 1])\r\n    for i, v in enumerate(pr_aucs):\r\n        axes[1, 1].text(i, v + 0.02, f'{v:.3f}', ha='center')\r\n    \r\n    plt.tight_layout()\r\n    plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=1.02)\r\n    plt.show()\r\n\r\nplot_model_comparison(evaluation_results)\r\n\r\n# Winner: LightGBM typically performs best for fraud detection\r\nprint(\"\\n\ud83c\udfc6 Recommended Model: LightGBM\")\r\nprint(\"   - Best PR-AUC (handles imbalance naturally)\")\r\nprint(\"   - Fastest predictions (<10ms)\")\r\nprint(\"   - Built-in handling of missing values\")\r\nprint(\"   - Native support for class weights\")\n"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-markdown",children:"**Model Interpretability**\r\n\r\n```python\r\nimport shap\r\nimport warnings\r\nwarnings.filterwarnings('ignore')\r\n\r\n# Why SHAP? Banks need explanations!\r\n# \"Why did you block my transaction?\" must be answerable\r\n\r\ndef explain_predictions(model, X_train, X_test, sample_size=100):\r\n    \"\"\"\r\n    Create SHAP explanations for model predictions\r\n    \"\"\"\r\n    # Create explainer (TreeExplainer for tree-based models)\r\n    explainer = shap.TreeExplainer(model)\r\n    \r\n    # Get SHAP values for test sample\r\n    X_test_sample = X_test.sample(min(sample_size, len(X_test)), random_state=42)\r\n    shap_values = explainer.shap_values(X_test_sample)\r\n    \r\n    # For binary classification, shap_values is list of two arrays\r\n    # We want the values for fraud class (class 1)\r\n    if isinstance(shap_values, list):\r\n        shap_values_fraud = shap_values[1]\r\n    else:\r\n        shap_values_fraud = shap_values\r\n    \r\n    # 1. Global feature importance\r\n    plt.figure(figsize=(12, 6))\r\n    \r\n    # Summary plot (shows feature impact across all predictions)\r\n    shap.summary_plot(\r\n        shap_values_fraud, \r\n        X_test_sample,\r\n        plot_type=\"dot\",\r\n        max_display=20,\r\n        show=False\r\n    )\r\n    plt.title('SHAP Feature Importance (Global)')\r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 2. Bar plot of mean absolute SHAP values\r\n    plt.figure(figsize=(10, 6))\r\n    shap.summary_plot(\r\n        shap_values_fraud, \r\n        X_test_sample,\r\n        plot_type=\"bar\",\r\n        max_display=20,\r\n        show=False\r\n    )\r\n    plt.title('Mean |SHAP| Feature Importance')\r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 3. Waterfall plot for a single high-fraud prediction\r\n    high_fraud_idx = np.argsort(-model.predict_proba(X_test_sample)[:, 1])[0]\r\n    \r\n    plt.figure(figsize=(12, 6))\r\n    shap.waterfall_plot(\r\n        shap.Explanation(\r\n            values=shap_values_fraud[high_fraud_idx],\r\n            base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\r\n            data=X_test_sample.iloc[high_fraud_idx].values,\r\n            feature_names=X_test_sample.columns.tolist()\r\n        ),\r\n        show=False,\r\n        max_display=15\r\n    )\r\n    plt.title(f'Why Transaction #{high_fraud_idx} Was Flagged as Fraud')\r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 4. Dependence plots (how feature affects prediction)\r\n    top_features = np.argsort(np.abs(shap_values_fraud).mean(0))[-3:]\r\n    \r\n    for feat_idx in top_features:\r\n        feature = X_test_sample.columns[feat_idx]\r\n        plt.figure(figsize=(10, 4))\r\n        shap.dependence_plot(\r\n            feat_idx, \r\n            shap_values_fraud, \r\n            X_test_sample,\r\n            display_features=X_test_sample,\r\n            show=False\r\n        )\r\n        plt.title(f'SHAP Dependence: {feature}')\r\n        plt.tight_layout()\r\n        plt.show()\r\n    \r\n    return explainer, shap_values_fraud\r\n\r\n# Run explanations\r\nexplainer, shap_values = explain_predictions(\r\n    model_lgb, \r\n    X_train, \r\n    X_test, \r\n    sample_size=1000\r\n)\r\n\r\n# Create a simple explanation function for API\r\ndef explain_transaction(transaction_dict, model, explainer, scaler):\r\n    \"\"\"\r\n    Generate human-readable explanation for a transaction\r\n    \"\"\"\r\n    # Convert to DataFrame and preprocess\r\n    df_trans = pd.DataFrame([transaction_dict])\r\n    \r\n    # Apply same preprocessing as training\r\n    df_trans['Scaled_Amount'] = scaler.transform(df_trans[['Amount']])\r\n    df_trans = df_trans.drop('Amount', axis=1)\r\n    \r\n    # Add time features\r\n    hour = (df_trans['Time'] / 3600) % 24\r\n    df_trans['Hour_sin'] = np.sin(2 * np.pi * hour/24)\r\n    df_trans['Hour_cos'] = np.cos(2 * np.pi * hour/24)\r\n    \r\n    # Get SHAP values for this transaction\r\n    shap_val = explainer.shap_values(df_trans)\r\n    if isinstance(shap_val, list):\r\n        shap_val = shap_val[1]  # Fraud class\r\n    \r\n    # Get top 3 reasons\r\n    feature_names = df_trans.columns\r\n    shap_importance = np.abs(shap_val[0])\r\n    top_indices = np.argsort(shap_importance)[-3:][::-1]\r\n    \r\n    reasons = []\r\n    for idx in top_indices:\r\n        feature = feature_names[idx]\r\n        value = df_trans.iloc[0][feature]\r\n        impact = shap_val[0][idx]\r\n        \r\n        if impact > 0:\r\n            direction = \"increases\"\r\n            reason = f\"{feature} = {value:.4f} (strongly increases fraud risk)\"\r\n        else:\r\n            direction = \"decreases\"\r\n            reason = f\"{feature} = {value:.4f} (decreases fraud risk)\"\r\n        \r\n        reasons.append(reason)\r\n    \r\n    fraud_prob = model.predict_proba(df_trans)[0, 1]\r\n    \r\n    explanation = {\r\n        'fraud_probability': float(fraud_prob),\r\n        'risk_factors': reasons,\r\n        'action': 'BLOCK' if fraud_prob > 0.8 else 'REVIEW' if fraud_prob > 0.3 else 'APPROVE'\r\n    }\r\n    \r\n    return explanation\r\n\r\nprint(\"\\nSample explanation:\")\r\nsample_transaction = {\r\n    'Time': 3600 * 3,  # 3 AM\r\n    'Amount': 500.00,\r\n    'V1': -1.2,\r\n    'V2': 0.5,\r\n    # ... other V features would be here\r\n}\r\nfor i in range(3, 29):\r\n    sample_transaction[f'V{i}'] = 0\r\n\r\nexplanation = explain_transaction(sample_transaction, model_lgb, explainer, scaler)\r\nfor key, value in explanation.items():\r\n    print(f\"{key}: {value}\")\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Model Serialization and Deployment Preparation"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import joblib\r\nimport json\r\nfrom datetime import datetime\r\nimport os\r\n\r\ndef save_model_artifacts(model, scaler, feature_names, metrics, path='models/'):\r\n    \"\"\"\r\n    Save all necessary components for deployment\r\n    \"\"\"\r\n    # Create directory if doesn't exist\r\n    os.makedirs(path, exist_ok=True)\r\n    \r\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\r\n    \r\n    # 1. Save model\r\n    model_path = f\"{path}/fraud_model_{timestamp}.pkl\"\r\n    joblib.dump(model, model_path)\r\n    print(f\"Model saved: {model_path}\")\r\n    \r\n    # 2. Save scaler\r\n    scaler_path = f\"{path}/scaler_{timestamp}.pkl\"\r\n    joblib.dump(scaler, scaler_path)\r\n    print(f\"Scaler saved: {scaler_path}\")\r\n    \r\n    # 3. Save feature names\r\n    features_path = f\"{path}/features_{timestamp}.json\"\r\n    with open(features_path, 'w') as f:\r\n        json.dump(list(feature_names), f)\r\n    print(f\"Features saved: {features_path}\")\r\n    \r\n    # 4. Save model metadata\r\n    metadata = {\r\n        'model_type': type(model).__name__,\r\n        'training_date': timestamp,\r\n        'features': list(feature_names),\r\n        'metrics': metrics,\r\n        'threshold': metrics.get('best_cost_threshold', 0.5),\r\n        'n_features': len(feature_names)\r\n    }\r\n    \r\n    metadata_path = f\"{path}/metadata_{timestamp}.json\"\r\n    with open(metadata_path, 'w') as f:\r\n        json.dump(metadata, f, indent=2)\r\n    print(f\"Metadata saved: {metadata_path}\")\r\n    \r\n    # 5. Save a sample configuration for API\r\n    config = {\r\n        'model_path': model_path,\r\n        'scaler_path': scaler_path,\r\n        'features_path': features_path,\r\n        'thresholds': {\r\n            'approve': 0.3,  # Below this, auto-approve\r\n            'review': 0.8,   # Between 0.3-0.8, manual review\r\n            'block': 0.8     # Above this, auto-block\r\n        }\r\n    }\r\n    \r\n    config_path = f\"{path}/deploy_config_{timestamp}.json\"\r\n    with open(config_path, 'w') as f:\r\n        json.dump(config, f, indent=2)\r\n    print(f\"Deployment config saved: {config_path}\")\r\n    \r\n    return {\r\n        'model_path': model_path,\r\n        'scaler_path': scaler_path,\r\n        'features_path': features_path,\r\n        'metadata_path': metadata_path,\r\n        'config_path': config_path\r\n    }\r\n\r\n# Prepare metrics dict\r\nmetrics = {\r\n    'best_cost_threshold': float(eval_results['best_cost_threshold']),\r\n    'pr_auc': float(eval_results['pr_auc']),\r\n    'avg_precision': float(eval_results['avg_precision']),\r\n    'roc_auc': float(eval_results['roc_auc']),\r\n    'min_cost': float(eval_results['min_cost'])\r\n}\r\n\r\n# Save everything\r\nsaved_paths = save_model_artifacts(\r\n    model_lgb, \r\n    scaler, \r\n    X_train.columns,\r\n    metrics\r\n)\r\n\r\n# Save a lightweight version for fast inference\r\ndef save_lightweight_model(model, path='models/lightweight/'):\r\n    \"\"\"\r\n    Save model in multiple formats for different deployment scenarios\r\n    \"\"\"\r\n    os.makedirs(path, exist_ok=True)\r\n    \r\n    # 1. Joblib format (fastest for Python)\r\n    joblib.dump(model, f\"{path}/model.joblib\")\r\n    \r\n    # 2. Try saving as ONNX for cross-platform deployment\r\n    try:\r\n        from skl2onnx import convert_sklearn\r\n        from skl2onnx.common.data_types import FloatTensorType\r\n        \r\n        # This is simplified - real ONNX conversion needs proper types\r\n        initial_type = [('float_input', FloatTensorType([None, X_train.shape[1]]))]\r\n        onx = convert_sklearn(model, initial_types=initial_type)\r\n        \r\n        with open(f\"{path}/model.onnx\", \"wb\") as f:\r\n            f.write(onx.SerializeToString())\r\n        print(\"ONNX model saved\")\r\n    except:\r\n        print(\"ONNX conversion skipped (install skl2onnx if needed)\")\r\n    \r\n    # 3. Save model parameters as JSON (for inspection)\r\n    if hasattr(model, 'get_params'):\r\n        params = model.get_params()\r\n        with open(f\"{path}/model_params.json\", 'w') as f:\r\n            json.dump({k: str(v) for k, v in params.items()}, f, indent=2)\r\n\r\nsave_lightweight_model(model_lgb)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Model monitoring and retraining strategy"})}),"\n",(0,t.jsx)(e.p,{children:"Fraud patterns evolve constantly. A model that works today may fail tomorrow. Here's a production monitoring system:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"def create_monitoring_dashboard():\r\n    \"\"\"\r\n    Track model performance in production\r\n    \"\"\"\r\n    monitoring_metrics = {\r\n        'daily': {\r\n            'transactions_processed': 0,\r\n            'avg_response_time_ms': 0,\r\n            'fraud_rate': 0.0,\r\n            'block_rate': 0.0,\r\n            'review_rate': 0.0\r\n        },\r\n        'drift_detection': {\r\n            'feature_drift_scores': {},  # PSI for each feature\r\n            'concept_drift': 0.0,        # Model performance degradation\r\n            'last_retrain_date': None\r\n        },\r\n        'business_metrics': {\r\n            'chargeback_rate': 0.0,\r\n            'customer_complaints': 0,\r\n            'false_positive_cost': 0.0\r\n        }\r\n    }\r\n    return monitoring_metrics\r\n\r\n### 2. Drift Detection System\r\n\r\nimport scipy.stats as stats\r\nfrom datetime import datetime, timedelta\r\n\r\ndef calculate_psi(expected_distribution, actual_distribution, buckets=10):\r\n    \"\"\"\r\n    Population Stability Index - measures feature drift\r\n    PSI < 0.1: No drift\r\n    PSI 0.1-0.2: Minor drift (monitor)\r\n    PSI > 0.2: Major drift (retrain needed)\r\n    \"\"\"\r\n    psi_value = 0\r\n    for i in range(buckets):\r\n        expected_perc = expected_distribution[i]\r\n        actual_perc = actual_distribution[i]\r\n        \r\n        # Avoid division by zero\r\n        if expected_perc == 0:\r\n            expected_perc = 0.0001\r\n        if actual_perc == 0:\r\n            actual_perc = 0.0001\r\n            \r\n        psi_value += (actual_perc - expected_perc) * np.log(actual_perc / expected_perc)\r\n    \r\n    return psi_value\r\n\r\ndef monitor_feature_drift(reference_data, current_data, features):\r\n    \"\"\"\r\n    Track drift for important features\r\n    \"\"\"\r\n    drift_report = {}\r\n    \r\n    for feature in features:\r\n        # Create distributions\r\n        ref_hist, bins = np.histogram(reference_data[feature], bins=10)\r\n        curr_hist, _ = np.histogram(current_data[feature], bins=bins)\r\n        \r\n        # Convert to percentages\r\n        ref_pct = ref_hist / len(reference_data)\r\n        curr_pct = curr_hist / len(current_data)\r\n        \r\n        # Calculate PSI\r\n        psi = calculate_psi(ref_pct, curr_pct)\r\n        \r\n        drift_report[feature] = {\r\n            'psi': psi,\r\n            'drift_level': 'MAJOR' if psi > 0.2 else 'MINOR' if psi > 0.1 else 'NONE',\r\n            'action': 'RETRAIN' if psi > 0.2 else 'MONITOR' if psi > 0.1 else 'OK'\r\n        }\r\n    \r\n    return drift_report\r\n\r\n### 3. Automated Retraining Pipeline\r\n\r\nclass FraudModelRetrainer:\r\n    \"\"\"\r\n    Automated model retraining with validation\r\n    \"\"\"\r\n    def __init__(self, model_path, performance_threshold=0.8):\r\n        self.model_path = model_path\r\n        self.performance_threshold = performance_threshold\r\n        self.retrain_history = []\r\n    \r\n    def should_retrain(self, current_metrics, days_since_train):\r\n        \"\"\"\r\n        Decision logic for when to retrain\r\n        \"\"\"\r\n        reasons = []\r\n        \r\n        # Time-based retraining (monthly)\r\n        if days_since_train > 30:\r\n            reasons.append(\"Monthly scheduled retraining\")\r\n        \r\n        # Performance degradation\r\n        if current_metrics['avg_precision'] < self.performance_threshold:\r\n            reasons.append(f\"Performance dropped: {current_metrics['avg_precision']:.3f} < {self.performance_threshold}\")\r\n        \r\n        # Feature drift\r\n        if current_metrics.get('max_feature_drift', 0) > 0.2:\r\n            reasons.append(f\"Feature drift detected: PSI={current_metrics['max_feature_drift']:.2f}\")\r\n        \r\n        # Business metric degradation\r\n        if current_metrics.get('false_positive_rate', 0) > 0.05:\r\n            reasons.append(f\"False positives too high: {current_metrics['false_positive_rate']:.2%}\")\r\n        \r\n        return {\r\n            'retrain': len(reasons) > 0,\r\n            'reasons': reasons\r\n        }\r\n    \r\n    def retrain_pipeline(self, new_data, new_labels, feature_names):\r\n        \"\"\"\r\n        Execute retraining with validation\r\n        \"\"\"\r\n        print(\"\ud83d\udd04 Starting model retraining...\")\r\n        \r\n        # Split new data\r\n        X_new_train, X_new_val, y_new_train, y_new_val = train_test_split(\r\n            new_data, new_labels, test_size=0.2, stratify=new_labels, random_state=42\r\n        )\r\n        \r\n        # Train new model\r\n        scale_pos_weight = len(y_new_train[y_new_train==0]) / len(y_new_train[y_new_train==1])\r\n        \r\n        new_model = lgb.LGBMClassifier(\r\n            n_estimators=500,\r\n            learning_rate=0.05,\r\n            scale_pos_weight=scale_pos_weight,\r\n            random_state=42\r\n        )\r\n        \r\n        new_model.fit(\r\n            X_new_train, y_new_train,\r\n            eval_set=[(X_new_val, y_new_val)],\r\n            callbacks=[lgb.early_stopping(50)]\r\n        )\r\n        \r\n        # Validate new model\r\n        val_pred = new_model.predict(X_new_val)\r\n        val_proba = new_model.predict_proba(X_new_val)[:, 1]\r\n        \r\n        new_performance = {\r\n            'precision': precision_score(y_new_val, val_pred),\r\n            'recall': recall_score(y_new_val, val_pred),\r\n            'avg_precision': average_precision_score(y_new_val, val_proba)\r\n        }\r\n        \r\n        # Compare with current model\r\n        current_model = joblib.load(f\"{self.model_path}/latest_model.pkl\")\r\n        current_pred = current_model.predict(X_new_val)\r\n        current_performance = {\r\n            'precision': precision_score(y_new_val, current_pred),\r\n            'recall': recall_score(y_new_val, current_pred),\r\n            'avg_precision': average_precision_score(y_new_val, current_model.predict_proba(X_new_val)[:, 1])\r\n        }\r\n        \r\n        # Decide whether to deploy\r\n        performance_improvement = new_performance['avg_precision'] - current_performance['avg_precision']\r\n        \r\n        retrain_record = {\r\n            'timestamp': datetime.now(),\r\n            'new_model_performance': new_performance,\r\n            'current_model_performance': current_performance,\r\n            'improvement': performance_improvement,\r\n            'deployed': performance_improvement > 0.01  # Deploy if >1% improvement\r\n        }\r\n        \r\n        if performance_improvement > 0.01:\r\n            # Save new model\r\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\r\n            model_path = f\"{self.model_path}/fraud_model_{timestamp}.pkl\"\r\n            joblib.dump(new_model, model_path)\r\n            \r\n            # Update symlink or config to point to new model\r\n            with open(f\"{self.model_path}/latest_model.txt\", 'w') as f:\r\n                f.write(f\"fraud_model_{timestamp}.pkl\")\r\n            \r\n            print(f\"\u2705 New model deployed! Improvement: {performance_improvement:.2%}\")\r\n        else:\r\n            print(f\"\u23f8\ufe0f Keeping current model. Improvement insufficient: {performance_improvement:.2%}\")\r\n        \r\n        self.retrain_history.append(retrain_record)\r\n        return retrain_record\r\n\r\n### 4. Alerting System\r\n\r\ndef create_alerts_config():\r\n    \"\"\"\r\n    Configure alerts for model issues\r\n    \"\"\"\r\n    alerts = {\r\n        'critical': {\r\n            'response_time_above_100ms': 'slack #fraud-alerts-critical',\r\n            'fraud_rate_drop_50%': 'pagerduty',\r\n            'api_error_rate_above_5%': 'email cto@company.com'\r\n        },\r\n        'warning': {\r\n            'feature_drift_above_0.15': 'slack #fraud-alerts',\r\n            'false_positives_up_20%': 'email fraud-team@company.com',\r\n            'model_accuracy_drop': 'jira ticket'\r\n        },\r\n        'info': {\r\n            'daily_report': 'email dashboard@company.com',\r\n            'weekly_retraining_summary': 'confluence page'\r\n        }\r\n    }\r\n    return alerts\r\n\r\n### 5. Production Monitoring Script\r\n\r\ndef production_monitoring_loop():\r\n    \"\"\"\r\n    Continuous monitoring (run every hour)\r\n    \"\"\"\r\n    while True:\r\n        try:\r\n            # 1. Collect recent predictions (last 24h)\r\n            recent_data = get_recent_transactions(hours=24)\r\n            reference_data = get_training_data()  # Original training set\r\n            \r\n            # 2. Check feature drift\r\n            important_features = ['Scaled_Amount', 'V1', 'V2', 'V3', 'Hour_sin']\r\n            drift_report = monitor_feature_drift(reference_data, recent_data, important_features)\r\n            \r\n            # 3. Check performance (if labels available)\r\n            if has_labels(recent_data):\r\n                performance = calculate_performance(recent_data)\r\n                \r\n                # 4. Decision: retrain?\r\n                retrainer = FraudModelRetrainer(model_path='models/')\r\n                decision = retrainer.should_retrain(\r\n                    current_metrics=performance,\r\n                    days_since_train=get_days_since_last_train()\r\n                )\r\n                \r\n                if decision['retrain']:\r\n                    print(f\"\ud83d\udea8 Retraining triggered: {decision['reasons']}\")\r\n                    retrainer.retrain_pipeline(recent_data, recent_data['Class'], recent_data.columns)\r\n            \r\n            # 5. Send alerts if needed\r\n            check_alerts(drift_report, performance)\r\n            \r\n            # 6. Sleep for 1 hour\r\n            time.sleep(3600)\r\n            \r\n        except Exception as e:\r\n            print(f\"Monitoring error: {e}\")\r\n            time.sleep(300)  # Retry in 5 minutes\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Understanding key concepts"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Understanding Class Imbalance"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Why it's a problem:"})}),"\n",(0,t.jsx)(e.p,{children:'Most ML algorithms assume balanced classes. With 99.8% genuine transactions, a model can achieve 99.8% accuracy by simply predicting "genuine" for everything.'}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"solution"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:"Classweights"})}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# How class weights work\r\nweights = {\r\n    0: 1.0,  # Genuine: normal weight\r\n    1: 100.0 # Fraud: 100x more important\r\n}\r\n# Model penalizes fraud misclassification 100x more\n"})}),"\n",(0,t.jsxs)(e.ol,{start:"2",children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:"Synthetic Minority Over-sampling"})}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# SMOTE creates synthetic fraud examples by:\r\n# 1. Pick a fraud transaction\r\n# 2. Find its k nearest neighbors (also fraud)\r\n# 3. Create new sample by interpolating between them\r\n# \r\n# Example: If fraud A = [1,2,3] and fraud B = [2,3,4]\r\n# New sample = [1.5, 2.5, 3.5]\n"})}),"\n",(0,t.jsxs)(e.ol,{start:"3",children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:"Undersampling"})}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Randomly sample genuine transactions to match fraud count\r\ngenuine_sample = genuine.sample(n=len(fraud), random_state=42)\r\nbalanced_data = pd.concat([genuine_sample, fraud])\r\n# Problem: Discards 99.6% of genuine data!\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Evolution metrics"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Why not accurate"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"Accuracy = (TP + TN) / (TP + TN + FP + FN)\r\n\r\nWith 99.8% genuine:\r\n- Predict all genuine: 99.8% accuracy\r\n- But catches 0% fraud!\n"})}),"\n",(0,t.jsxs)(e.p,{children:["*",(0,t.jsx)(e.strong,{children:"Precision"})]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"Precision = TP / (TP + FP)\r\nQuestion: Of transactions flagged as fraud, how many were actually fraud?\r\nBusiness impact: Low precision = annoying customers (false blocks)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Recall:"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"Recall = TP / (TP + FN)\r\nQuestion: Of actual fraud transactions, how many did we catch?\r\nBusiness impact: Low recall = losing money (missed fraud)\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"F1 Score"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"F1 = 2 * (Precision * Recall) / (Precision + Recall)\r\nHarmonic mean of precision and recall\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"PR-AUC (Precision-Recall Area Under Curve):"})}),"\n",(0,t.jsx)(e.p,{children:"Best metric for imbalanced data"}),"\n",(0,t.jsx)(e.p,{children:"Shows trade-off between precision and recall at all thresholds"}),"\n",(0,t.jsx)(e.p,{children:"Random model = PR-AUC = fraud rate (0.0017)"}),"\n",(0,t.jsx)(e.p,{children:"Perfect model = PR-AUC = 1.0"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Threshold Selection Strategies"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"def select_threshold_business_context(y_true, y_pred_proba, context):\r\n    \"\"\"\r\n    Select threshold based on business context\r\n    \"\"\"\r\n    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\r\n    \r\n    if context == 'high_risk':\r\n        # Banks losing lots of money \u2192 maximize recall\r\n        target_recall = 0.95\r\n        idx = np.argmin(np.abs(recall[:-1] - target_recall))\r\n        return thresholds[idx]\r\n    \r\n    elif context == 'customer_sensitive':\r\n        # Premium customers \u2192 maximize precision\r\n        target_precision = 0.99\r\n        idx = np.argmin(np.abs(precision[:-1] - target_precision))\r\n        return thresholds[idx]\r\n    \r\n    elif context == 'balanced':\r\n        # Standard scenario \u2192 maximize F1\r\n        f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-10)\r\n        return thresholds[np.argmax(f1_scores)]\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Feature Engineering"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Time-based features"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Problem: Time is cyclical (23:00 and 01:00 are close numerically)\r\n# Solution: Use sine/cosine transformation\r\n\r\ndef create_cyclical_features(hour):\r\n    """\r\n    Convert linear hour to cyclical features\r\n    """\r\n    radians = 2 * np.pi * hour / 24\r\n    return np.sin(radians), np.cos(radians)\r\n\r\n# Now 23:00 (sin= -0.5, cos=0.87) and 01:00 (sin=0.26, cos=0.97) are close!\n'})}),"\n",(0,t.jsxs)(e.p,{children:["*",(0,t.jsx)(e.strong,{children:"Velocity features"})]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def create_velocity_features(transactions, time_window=3600):\r\n    """\r\n    Count transactions in last hour for each transaction\r\n    """\r\n    velocities = []\r\n    for i, trans_time in enumerate(transactions[\'Time\']):\r\n        # Count transactions in last hour\r\n        count = len(transactions[\r\n            (transactions[\'Time\'] > trans_time - time_window) & \n'})})]})}function p(r={}){const{wrapper:e}={...(0,s.R)(),...r.components};return e?(0,t.jsx)(e,{...r,children:(0,t.jsx)(d,{...r})}):d(r)}},8741(r,e,n){n.d(e,{A:()=>a});const a=n.p+"assets/images/z-c7cb02bfd26d2fe7aee3e80a96c21e5e.png"}}]);